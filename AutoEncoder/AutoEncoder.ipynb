{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import elements from pytorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "movies = pd.read_csv(\"ml-1m/movies.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import users data\n",
    "users = pd.read_csv(\"ml-1m/users.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000209, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import ratings\n",
    "ratings = movies = pd.read_csv(\"ml-1m/ratings.dat\", sep = \"::\", header = None, engine = \"python\", encoding = \"latin-1\")\n",
    "ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>874965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2          3\n",
       "0  1  1  5  874965758\n",
       "1  1  2  3  876893171\n",
       "2  1  3  4  878542960\n",
       "3  1  4  3  876893119\n",
       "4  1  5  3  889751712"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import training and test sets\n",
    "training_set = pd.read_csv(\"ml-100k/u1.base\", delimiter = \"\\t\", header = None)\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to array\n",
    "training_set = np.array(training_set1, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same for test data\n",
    "test_set = pd.read_csv(\"ml-100k/u1.test\", delimiter = \"\\t\", header = None)\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943\n",
      "1682\n"
     ]
    }
   ],
   "source": [
    "# Find the total number of users and movies in the full dataset (training + test)\n",
    "combined = np.append(training_set, test_set, axis=0)\n",
    "nb_users = len(np.unique(combined[:,0]))\n",
    "nb_movies = len(np.unique(combined[:,1]))\n",
    "print(nb_users)\n",
    "print(nb_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert data into array such that users are rows and movies are columns\n",
    "# Torch takes input as list of lists\n",
    "def convert(data):\n",
    "    # Create list of lists\n",
    "    # Each element is a list of all ratings\n",
    "    # where rating at ith index belongs to ith movie\n",
    "    new_data = []\n",
    "    \n",
    "    for user_id in range(1, nb_users + 1):\n",
    "        movie_id = data[:,1][data[:,0] == user_id] # all movies for this user\n",
    "        rating = data[:,2][data[:,0] == user_id] # all ratings for this user\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[movie_id - 1] = rating # Indexing starts at zero but first movie id is 1\n",
    "        new_data.append(ratings)\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to tensors for torch inputs\n",
    "training_set_tensor = torch.FloatTensor(training_set)\n",
    "test_set_tensor = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create architecture of AutoEncoder\n",
    "# Parent class = nn.Module from pytorch\n",
    "# SAE inherits nn.Module\n",
    "class SAE(nn.Module):\n",
    "    # Init function\n",
    "    # blank space after self means it'll inherit variables from its parent class\n",
    "    def __init__(self, ):\n",
    "        # Get all inherited methods and variables from parent class by using super()\n",
    "        super(SAE, self).__init__()\n",
    "        # Specify full connection\n",
    "        self.fc1 = nn.Linear(nb_movies, 20) # nn.Linear as arguments takes no. of nodes in input layer and first hidden layer\n",
    "        # Build a stacked second hidden layer\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        # Build a stacked third hidden layer\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        # Build a layer to decode\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        # Activation layer\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    # Function to encode and decode in forward pass\n",
    "    # Takes as argument vector of input values\n",
    "    # Returns output vector\n",
    "    def forward(self, x):\n",
    "        # Step 1 - encoding\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        # Step 2 - decoding\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x) # No need of activation function\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the object\n",
    "sae = SAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring loss\n",
    "criterion = nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.7560476249194306\n",
      "epoch: 2 loss: 1.0963511341932897\n",
      "epoch: 3 loss: 1.0533084924779916\n",
      "epoch: 4 loss: 1.0385673262838446\n",
      "epoch: 5 loss: 1.0307154469200652\n",
      "epoch: 6 loss: 1.0265841390145418\n",
      "epoch: 7 loss: 1.0239332956573293\n",
      "epoch: 8 loss: 1.0220311394883255\n",
      "epoch: 9 loss: 1.0207079674902386\n",
      "epoch: 10 loss: 1.0194161198519744\n",
      "epoch: 11 loss: 1.0187327835902087\n",
      "epoch: 12 loss: 1.0181741513349465\n",
      "epoch: 13 loss: 1.0177639869003754\n",
      "epoch: 14 loss: 1.0176088117514397\n",
      "epoch: 15 loss: 1.0171605387925604\n",
      "epoch: 16 loss: 1.0167779710636324\n",
      "epoch: 17 loss: 1.0167184067995754\n",
      "epoch: 18 loss: 1.0165444675370403\n",
      "epoch: 19 loss: 1.0164219814100404\n",
      "epoch: 20 loss: 1.0160958218972944\n",
      "epoch: 21 loss: 1.0157835409156006\n",
      "epoch: 22 loss: 1.015929542796188\n",
      "epoch: 23 loss: 1.0158688907859645\n",
      "epoch: 24 loss: 1.0155583450894887\n",
      "epoch: 25 loss: 1.0156341881080129\n",
      "epoch: 26 loss: 1.0156159906322564\n",
      "epoch: 27 loss: 1.0153234531233255\n",
      "epoch: 28 loss: 1.0151503474808523\n",
      "epoch: 29 loss: 1.0126079625438276\n",
      "epoch: 30 loss: 1.0115518316641199\n",
      "epoch: 31 loss: 1.0099178909854962\n",
      "epoch: 32 loss: 1.0078542666542107\n",
      "epoch: 33 loss: 1.0080600201379255\n",
      "epoch: 34 loss: 1.0036864491807853\n",
      "epoch: 35 loss: 1.0027466948762325\n",
      "epoch: 36 loss: 0.9995373498739726\n",
      "epoch: 37 loss: 0.9997812721929008\n",
      "epoch: 38 loss: 0.9969002831187908\n",
      "epoch: 39 loss: 0.9962041694484829\n",
      "epoch: 40 loss: 0.993167061559746\n",
      "epoch: 41 loss: 0.9921845175435177\n",
      "epoch: 42 loss: 0.989865957579648\n",
      "epoch: 43 loss: 0.9885496970579014\n",
      "epoch: 44 loss: 0.9846339768838986\n",
      "epoch: 45 loss: 0.9821215105192794\n",
      "epoch: 46 loss: 0.9774476517926696\n",
      "epoch: 47 loss: 0.9794765223389906\n",
      "epoch: 48 loss: 0.9821916109529251\n",
      "epoch: 49 loss: 0.978634211263787\n",
      "epoch: 50 loss: 0.9747780974945979\n",
      "epoch: 51 loss: 0.9773923105291704\n",
      "epoch: 52 loss: 0.9799640720620563\n",
      "epoch: 53 loss: 0.9829767849801055\n",
      "epoch: 54 loss: 0.9796303101992501\n",
      "epoch: 55 loss: 0.9807445783810748\n",
      "epoch: 56 loss: 0.9797930784887119\n",
      "epoch: 57 loss: 0.9814960434371135\n",
      "epoch: 58 loss: 0.9783195305182145\n",
      "epoch: 59 loss: 0.9785956024236888\n",
      "epoch: 60 loss: 0.97785297842156\n",
      "epoch: 61 loss: 0.9735855757542842\n",
      "epoch: 62 loss: 0.967270689594255\n",
      "epoch: 63 loss: 0.9685396223473077\n",
      "epoch: 64 loss: 0.9643394126879429\n",
      "epoch: 65 loss: 0.965295539075166\n",
      "epoch: 66 loss: 0.9666438546645997\n",
      "epoch: 67 loss: 0.964461615999673\n",
      "epoch: 68 loss: 0.9652455895222718\n",
      "epoch: 69 loss: 0.9617766987738874\n",
      "epoch: 70 loss: 0.9570700000381136\n",
      "epoch: 71 loss: 0.9556139152479581\n",
      "epoch: 72 loss: 0.9562557414447901\n",
      "epoch: 73 loss: 0.960260478582487\n",
      "epoch: 74 loss: 0.9602313298624845\n",
      "epoch: 75 loss: 0.9578658951359669\n",
      "epoch: 76 loss: 0.9559101655909403\n",
      "epoch: 77 loss: 0.9570159514015149\n",
      "epoch: 78 loss: 0.9531055972375438\n",
      "epoch: 79 loss: 0.9528061892705079\n",
      "epoch: 80 loss: 0.9496921083596941\n",
      "epoch: 81 loss: 0.9507551184842488\n",
      "epoch: 82 loss: 0.9496368941148992\n",
      "epoch: 83 loss: 0.9484865267184178\n",
      "epoch: 84 loss: 0.9446424728820025\n",
      "epoch: 85 loss: 0.9465220650641324\n",
      "epoch: 86 loss: 0.9452240378272956\n",
      "epoch: 87 loss: 0.9447978334570954\n",
      "epoch: 88 loss: 0.9456342423556153\n",
      "epoch: 89 loss: 0.9475780036405244\n",
      "epoch: 90 loss: 0.94412486945503\n",
      "epoch: 91 loss: 0.94416435787966\n",
      "epoch: 92 loss: 0.9429534893488244\n",
      "epoch: 93 loss: 0.9428311143692983\n",
      "epoch: 94 loss: 0.9415576967702951\n",
      "epoch: 95 loss: 0.941365648001547\n",
      "epoch: 96 loss: 0.9394506123923687\n",
      "epoch: 97 loss: 0.9385282671063135\n",
      "epoch: 98 loss: 0.937846214748904\n",
      "epoch: 99 loss: 0.9386358042955293\n",
      "epoch: 100 loss: 0.936082656043264\n",
      "epoch: 101 loss: 0.9365973376519993\n",
      "epoch: 102 loss: 0.935199585662341\n",
      "epoch: 103 loss: 0.9366076001075265\n",
      "epoch: 104 loss: 0.935750013416576\n",
      "epoch: 105 loss: 0.9348097544882761\n",
      "epoch: 106 loss: 0.933223241939926\n",
      "epoch: 107 loss: 0.9337490729760262\n",
      "epoch: 108 loss: 0.9321898328945494\n",
      "epoch: 109 loss: 0.9329832004380108\n",
      "epoch: 110 loss: 0.9309548277258247\n",
      "epoch: 111 loss: 0.932649450035408\n",
      "epoch: 112 loss: 0.9305610165897596\n",
      "epoch: 113 loss: 0.931012975363631\n",
      "epoch: 114 loss: 0.9297041730714023\n",
      "epoch: 115 loss: 0.9301515845066148\n",
      "epoch: 116 loss: 0.9287212415156295\n",
      "epoch: 117 loss: 0.9293000431414212\n",
      "epoch: 118 loss: 0.9278998910215519\n",
      "epoch: 119 loss: 0.9281578039681774\n",
      "epoch: 120 loss: 0.9271755803507458\n",
      "epoch: 121 loss: 0.9274432012043595\n",
      "epoch: 122 loss: 0.9263348654804251\n",
      "epoch: 123 loss: 0.9265236798019366\n",
      "epoch: 124 loss: 0.925698044651404\n",
      "epoch: 125 loss: 0.926082501017991\n",
      "epoch: 126 loss: 0.9247357846309422\n",
      "epoch: 127 loss: 0.9249667823467124\n",
      "epoch: 128 loss: 0.9238142295356084\n",
      "epoch: 129 loss: 0.9247323112870148\n",
      "epoch: 130 loss: 0.9242297288359773\n",
      "epoch: 131 loss: 0.9242455165488361\n",
      "epoch: 132 loss: 0.9234584549775859\n",
      "epoch: 133 loss: 0.9234961425372756\n",
      "epoch: 134 loss: 0.9225926311029355\n",
      "epoch: 135 loss: 0.9227243200355387\n",
      "epoch: 136 loss: 0.9218267867441279\n",
      "epoch: 137 loss: 0.9218508461313303\n",
      "epoch: 138 loss: 0.9216045246591323\n",
      "epoch: 139 loss: 0.9218469005531399\n",
      "epoch: 140 loss: 0.9212925165664475\n",
      "epoch: 141 loss: 0.921286705181943\n",
      "epoch: 142 loss: 0.9204993575648831\n",
      "epoch: 143 loss: 0.9205705421876347\n",
      "epoch: 144 loss: 0.9196937444489682\n",
      "epoch: 145 loss: 0.9201150401153044\n",
      "epoch: 146 loss: 0.919870622594853\n",
      "epoch: 147 loss: 0.9202487263049017\n",
      "epoch: 148 loss: 0.9200868277197669\n",
      "epoch: 149 loss: 0.9197410931341229\n",
      "epoch: 150 loss: 0.9193469675613221\n",
      "epoch: 151 loss: 0.9193661528597133\n",
      "epoch: 152 loss: 0.9184611735083219\n",
      "epoch: 153 loss: 0.9186390359577145\n",
      "epoch: 154 loss: 0.9186936351450559\n",
      "epoch: 155 loss: 0.9184350298785106\n",
      "epoch: 156 loss: 0.918420863376698\n",
      "epoch: 157 loss: 0.9180725949768002\n",
      "epoch: 158 loss: 0.917550496711405\n",
      "epoch: 159 loss: 0.917581229358\n",
      "epoch: 160 loss: 0.9172006943863484\n",
      "epoch: 161 loss: 0.9177864782389671\n",
      "epoch: 162 loss: 0.9177952419335108\n",
      "epoch: 163 loss: 0.9177995617097454\n",
      "epoch: 164 loss: 0.9174153820621521\n",
      "epoch: 165 loss: 0.9172678357058456\n",
      "epoch: 166 loss: 0.9163595577880027\n",
      "epoch: 167 loss: 0.9167502030259397\n",
      "epoch: 168 loss: 0.9162679443859121\n",
      "epoch: 169 loss: 0.9169294827576616\n",
      "epoch: 170 loss: 0.9164228423570067\n",
      "epoch: 171 loss: 0.9161722395293672\n",
      "epoch: 172 loss: 0.9162187020030614\n",
      "epoch: 173 loss: 0.9159521666954812\n",
      "epoch: 174 loss: 0.9157220452060573\n",
      "epoch: 175 loss: 0.9154952923410551\n",
      "epoch: 176 loss: 0.9151524574631746\n",
      "epoch: 177 loss: 0.9148942580491116\n",
      "epoch: 178 loss: 0.9149791071790063\n",
      "epoch: 179 loss: 0.9152919434182419\n",
      "epoch: 180 loss: 0.9147844871028443\n",
      "epoch: 181 loss: 0.9146711795082002\n",
      "epoch: 182 loss: 0.9146410841177499\n",
      "epoch: 183 loss: 0.9147308398079899\n",
      "epoch: 184 loss: 0.9144442550844358\n",
      "epoch: 185 loss: 0.9142994506364035\n",
      "epoch: 186 loss: 0.9139529829224662\n",
      "epoch: 187 loss: 0.9138699280237113\n",
      "epoch: 188 loss: 0.9135633422463135\n",
      "epoch: 189 loss: 0.9137837976444544\n",
      "epoch: 190 loss: 0.9135108822853061\n",
      "epoch: 191 loss: 0.9136677955004192\n",
      "epoch: 192 loss: 0.9133833666171411\n",
      "epoch: 193 loss: 0.9135183472904378\n",
      "epoch: 194 loss: 0.9131882514319049\n",
      "epoch: 195 loss: 0.9130534062039942\n",
      "epoch: 196 loss: 0.9129089703795704\n",
      "epoch: 197 loss: 0.9128223343652154\n",
      "epoch: 198 loss: 0.9125706384457855\n",
      "epoch: 199 loss: 0.9126366720550758\n",
      "epoch: 200 loss: 0.9125314059760447\n"
     ]
    }
   ],
   "source": [
    "# Train the SAE\n",
    "n_epoch = 200\n",
    "\n",
    "for epoch in range(1, n_epoch + 1):\n",
    "    train_loss = 0 # Average difference of actual rating and predicted rating\n",
    "    counter = 0.0\n",
    "    \n",
    "    # Loop over all users\n",
    "    for user in range(nb_users):\n",
    "        input_ = Variable(training_set_tensor[user]).unsqueeze(0) # Create extra dimension\n",
    "        target = input_.clone() # target set as input\n",
    "        \n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            # Take only those observations where none of the ratings given by a user is zero\n",
    "            # i.e the user has rated at least one movie\n",
    "                output = sae(input_)\n",
    "                # Don't compute gradient with target vector\n",
    "                target.require_grad = False\n",
    "                output[target == 0] = 0\n",
    "                loss = criterion(output, target)\n",
    "                # Compute average of error considering only those movies\n",
    "                # that got non-zero ratings\n",
    "                mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # Keep deonminator positive\n",
    "                loss.backward()\n",
    "                train_loss += np.sqrt(loss.item() * mean_corrector) # Adjusted loss\n",
    "                counter += 1\n",
    "                # Optimizer decides the amount by which updates happen\n",
    "                optimizer.step()\n",
    "                \n",
    "    print(\"epoch: \" + str(epoch) + \" loss: \" + str(train_loss/counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.874764958247081\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_loss = 0\n",
    "counter = 0.0\n",
    "# Loop over all users\n",
    "for user in range(nb_users):\n",
    "    input_ = Variable(training_set_tensor[user]).unsqueeze(0) # Create extra dimension\n",
    "    target = Variable(test_set_tensor[user]).unsqueeze(0) # target set as input\n",
    "        \n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        # Take only those observations where none of the ratings given by a user is zero\n",
    "        # i.e the user has rated at least one movie\n",
    "            output = sae(input_)\n",
    "            # Don't compute gradient with target vector\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            # Compute average of error considering only those movies\n",
    "            # that got non-zero ratings\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10) # Keep deonminator positive\n",
    "            test_loss += np.sqrt(loss.item() * mean_corrector) # Adjusted loss\n",
    "            counter += 1\n",
    "            \n",
    "print(\"loss: \" + str(train_loss/counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
